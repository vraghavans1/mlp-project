{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 99546,
          "databundleVersionId": 11895149,
          "sourceType": "competition"
        }
      ],
      "dockerImageVersionId": 31012,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vraghavans1/mlp-project/blob/main/Copy_of_22f3002203_notebook_t22025.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE\n",
        "# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.\n",
        "import kagglehub\n",
        "kagglehub.login()\n"
      ],
      "metadata": {
        "id": "s8gcwsnyrgET"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "engage_2_value_from_clicks_to_conversions_path = kagglehub.competition_download('engage-2-value-from-clicks-to-conversions')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "3NgcofUbrgEU"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.dummy import DummyRegressor\n",
        "import lightgbm as lgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-25T05:54:25.214939Z",
          "iopub.execute_input": "2025-05-25T05:54:25.215323Z",
          "iopub.status.idle": "2025-05-25T05:54:32.911872Z",
          "shell.execute_reply.started": "2025-05-25T05:54:25.215289Z",
          "shell.execute_reply": "2025-05-25T05:54:32.910943Z"
        },
        "id": "V5bmXnDorgEV"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DATA LOADING  "
      ],
      "metadata": {
        "id": "A18Y8s8YrgEV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data=pd.read_csv('train_data.csv')\n",
        "test_data=pd.read_csv('test_data.csv')\n",
        "sample_data=pd.read_csv('sample_submission.csv')"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-25T05:54:32.913496Z",
          "iopub.execute_input": "2025-05-25T05:54:32.914253Z",
          "iopub.status.idle": "2025-05-25T05:54:36.379788Z",
          "shell.execute_reply.started": "2025-05-25T05:54:32.91422Z",
          "shell.execute_reply": "2025-05-25T05:54:36.378653Z"
        },
        "id": "825NzlsgrgEW"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.head()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-25T05:54:36.381048Z",
          "iopub.execute_input": "2025-05-25T05:54:36.381709Z",
          "iopub.status.idle": "2025-05-25T05:54:36.437949Z",
          "shell.execute_reply.started": "2025-05-25T05:54:36.381665Z",
          "shell.execute_reply": "2025-05-25T05:54:36.436558Z"
        },
        "id": "x-5PBfDjrgEW",
        "outputId": "04c623b3-0d99-432e-a7f9-8ce03ced9980",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 342
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  trafficSource.isTrueDirect  purchaseValue            browser  \\\n",
              "0                        NaN            0.0               Edge   \n",
              "1                       True            0.0             Chrome   \n",
              "2                       True            0.0             Chrome   \n",
              "3                        NaN            0.0  Internet Explorer   \n",
              "4                       True     88950000.0             Chrome   \n",
              "\n",
              "         device.screenResolution trafficSource.adContent  \\\n",
              "0  not available in demo dataset                     NaN   \n",
              "1  not available in demo dataset                     NaN   \n",
              "2  not available in demo dataset                     NaN   \n",
              "3  not available in demo dataset                     NaN   \n",
              "4  not available in demo dataset                     NaN   \n",
              "\n",
              "  trafficSource.keyword screenSize geoCluster  \\\n",
              "0                   NaN     medium   Region_2   \n",
              "1                   NaN     medium   Region_3   \n",
              "2        (not provided)     medium   Region_2   \n",
              "3                   NaN     medium   Region_4   \n",
              "4                   NaN     medium   Region_3   \n",
              "\n",
              "  trafficSource.adwordsClickInfo.slot    device.mobileDeviceBranding  ...  \\\n",
              "0                                 NaN  not available in demo dataset  ...   \n",
              "1                                 NaN  not available in demo dataset  ...   \n",
              "2                                 NaN  not available in demo dataset  ...   \n",
              "3                                 NaN  not available in demo dataset  ...   \n",
              "4                                 NaN  not available in demo dataset  ...   \n",
              "\n",
              "                 device.language  deviceType     userChannel  \\\n",
              "0  not available in demo dataset     desktop          Social   \n",
              "1  not available in demo dataset     desktop          Direct   \n",
              "2  not available in demo dataset     desktop  Organic Search   \n",
              "3  not available in demo dataset     desktop          Social   \n",
              "4  not available in demo dataset     desktop          Direct   \n",
              "\n",
              "           device.browserVersion totalHits            device.screenColors  \\\n",
              "0  not available in demo dataset       1.0  not available in demo dataset   \n",
              "1  not available in demo dataset       1.0  not available in demo dataset   \n",
              "2  not available in demo dataset       6.0  not available in demo dataset   \n",
              "3  not available in demo dataset       1.0  not available in demo dataset   \n",
              "4  not available in demo dataset      66.0  not available in demo dataset   \n",
              "\n",
              "   sessionStart  geoNetwork.continent device.isMobile new_visits  \n",
              "0  1.500101e+09              Americas           False        1.0  \n",
              "1  1.495262e+09              Americas           False        1.0  \n",
              "2  1.508510e+09                Europe           False        NaN  \n",
              "3  1.483432e+09                  Asia           False        1.0  \n",
              "4  1.475805e+09              Americas           False        1.0  \n",
              "\n",
              "[5 rows x 52 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f86fe780-a33d-4aa7-82a6-854151e44d31\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>trafficSource.isTrueDirect</th>\n",
              "      <th>purchaseValue</th>\n",
              "      <th>browser</th>\n",
              "      <th>device.screenResolution</th>\n",
              "      <th>trafficSource.adContent</th>\n",
              "      <th>trafficSource.keyword</th>\n",
              "      <th>screenSize</th>\n",
              "      <th>geoCluster</th>\n",
              "      <th>trafficSource.adwordsClickInfo.slot</th>\n",
              "      <th>device.mobileDeviceBranding</th>\n",
              "      <th>...</th>\n",
              "      <th>device.language</th>\n",
              "      <th>deviceType</th>\n",
              "      <th>userChannel</th>\n",
              "      <th>device.browserVersion</th>\n",
              "      <th>totalHits</th>\n",
              "      <th>device.screenColors</th>\n",
              "      <th>sessionStart</th>\n",
              "      <th>geoNetwork.continent</th>\n",
              "      <th>device.isMobile</th>\n",
              "      <th>new_visits</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>Edge</td>\n",
              "      <td>not available in demo dataset</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>medium</td>\n",
              "      <td>Region_2</td>\n",
              "      <td>NaN</td>\n",
              "      <td>not available in demo dataset</td>\n",
              "      <td>...</td>\n",
              "      <td>not available in demo dataset</td>\n",
              "      <td>desktop</td>\n",
              "      <td>Social</td>\n",
              "      <td>not available in demo dataset</td>\n",
              "      <td>1.0</td>\n",
              "      <td>not available in demo dataset</td>\n",
              "      <td>1.500101e+09</td>\n",
              "      <td>Americas</td>\n",
              "      <td>False</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>True</td>\n",
              "      <td>0.0</td>\n",
              "      <td>Chrome</td>\n",
              "      <td>not available in demo dataset</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>medium</td>\n",
              "      <td>Region_3</td>\n",
              "      <td>NaN</td>\n",
              "      <td>not available in demo dataset</td>\n",
              "      <td>...</td>\n",
              "      <td>not available in demo dataset</td>\n",
              "      <td>desktop</td>\n",
              "      <td>Direct</td>\n",
              "      <td>not available in demo dataset</td>\n",
              "      <td>1.0</td>\n",
              "      <td>not available in demo dataset</td>\n",
              "      <td>1.495262e+09</td>\n",
              "      <td>Americas</td>\n",
              "      <td>False</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>True</td>\n",
              "      <td>0.0</td>\n",
              "      <td>Chrome</td>\n",
              "      <td>not available in demo dataset</td>\n",
              "      <td>NaN</td>\n",
              "      <td>(not provided)</td>\n",
              "      <td>medium</td>\n",
              "      <td>Region_2</td>\n",
              "      <td>NaN</td>\n",
              "      <td>not available in demo dataset</td>\n",
              "      <td>...</td>\n",
              "      <td>not available in demo dataset</td>\n",
              "      <td>desktop</td>\n",
              "      <td>Organic Search</td>\n",
              "      <td>not available in demo dataset</td>\n",
              "      <td>6.0</td>\n",
              "      <td>not available in demo dataset</td>\n",
              "      <td>1.508510e+09</td>\n",
              "      <td>Europe</td>\n",
              "      <td>False</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>Internet Explorer</td>\n",
              "      <td>not available in demo dataset</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>medium</td>\n",
              "      <td>Region_4</td>\n",
              "      <td>NaN</td>\n",
              "      <td>not available in demo dataset</td>\n",
              "      <td>...</td>\n",
              "      <td>not available in demo dataset</td>\n",
              "      <td>desktop</td>\n",
              "      <td>Social</td>\n",
              "      <td>not available in demo dataset</td>\n",
              "      <td>1.0</td>\n",
              "      <td>not available in demo dataset</td>\n",
              "      <td>1.483432e+09</td>\n",
              "      <td>Asia</td>\n",
              "      <td>False</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>True</td>\n",
              "      <td>88950000.0</td>\n",
              "      <td>Chrome</td>\n",
              "      <td>not available in demo dataset</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>medium</td>\n",
              "      <td>Region_3</td>\n",
              "      <td>NaN</td>\n",
              "      <td>not available in demo dataset</td>\n",
              "      <td>...</td>\n",
              "      <td>not available in demo dataset</td>\n",
              "      <td>desktop</td>\n",
              "      <td>Direct</td>\n",
              "      <td>not available in demo dataset</td>\n",
              "      <td>66.0</td>\n",
              "      <td>not available in demo dataset</td>\n",
              "      <td>1.475805e+09</td>\n",
              "      <td>Americas</td>\n",
              "      <td>False</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows √ó 52 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f86fe780-a33d-4aa7-82a6-854151e44d31')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-f86fe780-a33d-4aa7-82a6-854151e44d31 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-f86fe780-a33d-4aa7-82a6-854151e44d31');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-1f8affd0-e8e5-4977-9ed8-88bcd72a6bcd\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-1f8affd0-e8e5-4977-9ed8-88bcd72a6bcd')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-1f8affd0-e8e5-4977-9ed8-88bcd72a6bcd button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "train_data"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "test_data.head()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-25T05:54:36.439426Z",
          "iopub.execute_input": "2025-05-25T05:54:36.439782Z",
          "iopub.status.idle": "2025-05-25T05:54:36.474455Z",
          "shell.execute_reply.started": "2025-05-25T05:54:36.439751Z",
          "shell.execute_reply": "2025-05-25T05:54:36.473154Z"
        },
        "id": "tBJUGojHrgEW"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  EXPLORATORY DATA ANALYSIS"
      ],
      "metadata": {
        "id": "bUASxEFLrgEW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.shape, test_data.shape"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-25T05:54:36.476793Z",
          "iopub.execute_input": "2025-05-25T05:54:36.477135Z",
          "iopub.status.idle": "2025-05-25T05:54:36.493746Z",
          "shell.execute_reply.started": "2025-05-25T05:54:36.477096Z",
          "shell.execute_reply": "2025-05-25T05:54:36.492942Z"
        },
        "id": "QPxac4bDrgEW"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Summary of the Data set"
      ],
      "metadata": {
        "id": "H4EbQqfbrgEW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.info()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-25T05:54:36.494776Z",
          "iopub.execute_input": "2025-05-25T05:54:36.495056Z",
          "iopub.status.idle": "2025-05-25T05:54:36.747306Z",
          "shell.execute_reply.started": "2025-05-25T05:54:36.495026Z",
          "shell.execute_reply": "2025-05-25T05:54:36.74618Z"
        },
        "id": "GftmGsX8rgEW",
        "outputId": "a494c7d9-a7a4-47af-d45f-c3b96d903295",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 108350 entries, 0 to 108349\n",
            "Data columns (total 52 columns):\n",
            " #   Column                                        Non-Null Count   Dtype  \n",
            "---  ------                                        --------------   -----  \n",
            " 0   trafficSource.isTrueDirect                    40093 non-null   object \n",
            " 1   purchaseValue                                 108350 non-null  float64\n",
            " 2   browser                                       108350 non-null  object \n",
            " 3   device.screenResolution                       108350 non-null  object \n",
            " 4   trafficSource.adContent                       2750 non-null    object \n",
            " 5   trafficSource.keyword                         41144 non-null   object \n",
            " 6   screenSize                                    108350 non-null  object \n",
            " 7   geoCluster                                    108350 non-null  object \n",
            " 8   trafficSource.adwordsClickInfo.slot           3979 non-null    object \n",
            " 9   device.mobileDeviceBranding                   108350 non-null  object \n",
            " 10  device.mobileInputSelector                    108350 non-null  object \n",
            " 11  userId                                        108350 non-null  int64  \n",
            " 12  trafficSource.campaign                        108350 non-null  object \n",
            " 13  device.mobileDeviceMarketingName              108350 non-null  object \n",
            " 14  geoNetwork.networkDomain                      108349 non-null  object \n",
            " 15  gclIdPresent                                  108349 non-null  float64\n",
            " 16  device.operatingSystemVersion                 108349 non-null  object \n",
            " 17  sessionNumber                                 108349 non-null  float64\n",
            " 18  device.flashVersion                           108349 non-null  object \n",
            " 19  geoNetwork.region                             108349 non-null  object \n",
            " 20  trafficSource                                 108349 non-null  object \n",
            " 21  totals.visits                                 108349 non-null  float64\n",
            " 22  geoNetwork.networkLocation                    108349 non-null  object \n",
            " 23  sessionId                                     108349 non-null  float64\n",
            " 24  os                                            108349 non-null  object \n",
            " 25  geoNetwork.subContinent                       108349 non-null  object \n",
            " 26  trafficSource.medium                          108349 non-null  object \n",
            " 27  trafficSource.adwordsClickInfo.isVideoAd      3979 non-null    object \n",
            " 28  browserMajor                                  108349 non-null  object \n",
            " 29  locationCountry                               108349 non-null  object \n",
            " 30  device.browserSize                            108349 non-null  object \n",
            " 31  trafficSource.adwordsClickInfo.adNetworkType  3979 non-null    object \n",
            " 32  socialEngagementType                          108349 non-null  object \n",
            " 33  geoNetwork.city                               108349 non-null  object \n",
            " 34  trafficSource.adwordsClickInfo.page           3979 non-null    float64\n",
            " 35  geoNetwork.metro                              108349 non-null  object \n",
            " 36  pageViews                                     108341 non-null  float64\n",
            " 37  locationZone                                  108349 non-null  float64\n",
            " 38  device.mobileDeviceModel                      108349 non-null  object \n",
            " 39  trafficSource.referralPath                    39963 non-null   object \n",
            " 40  totals.bounces                                44022 non-null   float64\n",
            " 41  date                                          108349 non-null  float64\n",
            " 42  device.language                               108349 non-null  object \n",
            " 43  deviceType                                    108349 non-null  object \n",
            " 44  userChannel                                   108349 non-null  object \n",
            " 45  device.browserVersion                         108349 non-null  object \n",
            " 46  totalHits                                     108349 non-null  float64\n",
            " 47  device.screenColors                           108349 non-null  object \n",
            " 48  sessionStart                                  108349 non-null  float64\n",
            " 49  geoNetwork.continent                          108349 non-null  object \n",
            " 50  device.isMobile                               108349 non-null  object \n",
            " 51  new_visits                                    75219 non-null   float64\n",
            "dtypes: float64(13), int64(1), object(38)\n",
            "memory usage: 43.0+ MB\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "test_data.info()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-25T05:54:36.748382Z",
          "iopub.execute_input": "2025-05-25T05:54:36.748762Z",
          "iopub.status.idle": "2025-05-25T05:54:36.814657Z",
          "shell.execute_reply.started": "2025-05-25T05:54:36.748733Z",
          "shell.execute_reply": "2025-05-25T05:54:36.81378Z"
        },
        "id": "H_KUnZw4rgEX"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Identify missing values that may require imputation or removal."
      ],
      "metadata": {
        "id": "FTBj6iO_rgEX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "missing_values = train_data.isnull().sum().sort_values(ascending=False)\n",
        "print(missing_values[missing_values > 0])"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-25T05:54:36.815793Z",
          "iopub.execute_input": "2025-05-25T05:54:36.816534Z",
          "iopub.status.idle": "2025-05-25T05:54:37.031811Z",
          "shell.execute_reply.started": "2025-05-25T05:54:36.816498Z",
          "shell.execute_reply": "2025-05-25T05:54:37.030886Z"
        },
        "id": "judRI1VmrgEX"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate and sort the percentage of missing values per feature in descending order\n",
        "missing_percentage = (train_data.isnull().sum() / len(train_data)) * 100\n",
        "missing_percentage = missing_percentage[missing_percentage > 0].sort_values(ascending=False)\n",
        "# print(missing_percentage)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "missing_percentage.plot(kind='bar', color='red')\n",
        "plt.title('Percentage of Missing Values per Feature')\n",
        "plt.xlabel('Features')\n",
        "plt.ylabel('Missing Value Percentage')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-25T05:54:37.032729Z",
          "iopub.execute_input": "2025-05-25T05:54:37.032967Z",
          "iopub.status.idle": "2025-05-25T05:54:37.694885Z",
          "shell.execute_reply.started": "2025-05-25T05:54:37.032948Z",
          "shell.execute_reply": "2025-05-25T05:54:37.693855Z"
        },
        "id": "n7ib2KyrrgEX"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Null values in the Test data"
      ],
      "metadata": {
        "id": "F5UoTbyGrgEX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "display(test_data[test_data.columns[test_data.isnull().any()]].isnull().sum())"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-25T05:54:37.695996Z",
          "iopub.execute_input": "2025-05-25T05:54:37.696694Z",
          "iopub.status.idle": "2025-05-25T05:54:37.768451Z",
          "shell.execute_reply.started": "2025-05-25T05:54:37.696662Z",
          "shell.execute_reply": "2025-05-25T05:54:37.767209Z"
        },
        "id": "6F1MsFocrgEX"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "test_null_columns = set(test_data.columns[test_data.isnull().any()])\n",
        "train_null_columns = set(train_data.columns[train_data.isnull().any()])\n",
        "\n",
        "common_missing_columns = test_null_columns & train_null_columns\n",
        "\n",
        "test_only_missing_columns = test_null_columns - train_null_columns\n",
        "train_only_missing_columns = train_null_columns - test_null_columns\n",
        "\n",
        "print(\"Columns with missing values in BOTH train and test data:\")\n",
        "print(common_missing_columns)\n",
        "\n",
        "print(\"\\nColumns with missing values ONLY in test data:\")\n",
        "print(test_only_missing_columns)\n",
        "\n",
        "print(\"\\nColumns with missing values ONLY in train data:\")\n",
        "print(train_only_missing_columns)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-25T05:54:37.769694Z",
          "iopub.execute_input": "2025-05-25T05:54:37.769967Z",
          "iopub.status.idle": "2025-05-25T05:54:38.030733Z",
          "shell.execute_reply.started": "2025-05-25T05:54:37.769948Z",
          "shell.execute_reply": "2025-05-25T05:54:38.029843Z"
        },
        "id": "D9gqCtfVrgEY"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### All of the missing values are common to both training and test data"
      ],
      "metadata": {
        "id": "IgV6TRzUrgEY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Unique values in each column in the training data"
      ],
      "metadata": {
        "id": "idx9DIDSrgEY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.nunique().sort_values(ascending=False)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-25T05:54:38.031507Z",
          "iopub.execute_input": "2025-05-25T05:54:38.031734Z",
          "iopub.status.idle": "2025-05-25T05:54:38.319796Z",
          "shell.execute_reply.started": "2025-05-25T05:54:38.031716Z",
          "shell.execute_reply": "2025-05-25T05:54:38.318826Z"
        },
        "id": "qQ725v5FrgEY"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "top_10_cardinal_columns = train_data.nunique().sort_values(ascending=False).head(10)\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "top_10_cardinal_columns.plot(kind='bar', color='royalblue')\n",
        "\n",
        "# Customize plot\n",
        "plt.title(\"Top 10 High-Cardinality Columns\", fontsize=14)\n",
        "plt.xlabel(\"Columns\", fontsize=12)\n",
        "plt.ylabel(\"Unique Values Count\", fontsize=12)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-25T05:54:38.320809Z",
          "iopub.execute_input": "2025-05-25T05:54:38.321171Z",
          "iopub.status.idle": "2025-05-25T05:54:38.834339Z",
          "shell.execute_reply.started": "2025-05-25T05:54:38.321134Z",
          "shell.execute_reply": "2025-05-25T05:54:38.833416Z"
        },
        "id": "cKDdmXL1rgEY"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is the correlation between the number of pageviews and the purchase value, considering only users who did not bounce?"
      ],
      "metadata": {
        "id": "_To6W8ItrgEY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter out rows where purchase_value is 0\n",
        "filtered_data = train_data[train_data['purchaseValue'] != 0].copy()\n",
        "\n",
        "# Calculate the correlation\n",
        "correlation = filtered_data['pageViews'].corr(filtered_data['purchaseValue'])\n",
        "\n",
        "correlation"
      ],
      "metadata": {
        "trusted": true,
        "id": "MIRz0Id-rgEY",
        "outputId": "9e31dcb1-c57a-4c68-bcce-61708e60b631",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.float64(0.12191709637943975)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter data to include only users who purchased (purchaseValue > 0)\n",
        "purchased_users_data = train_data[train_data['purchaseValue'] > 0].copy()\n",
        "\n",
        "# Calculate the correlation between pageviews and purchase value for purchased users\n",
        "correlation_purchased = purchased_users_data['pageViews'].corr(purchased_users_data['purchaseValue'])\n",
        "correlation_purchased"
      ],
      "metadata": {
        "id": "5_O38dVnwQx2",
        "outputId": "be119e3f-1afb-4795-be69-2bbf49926f7b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.float64(0.12191709637943975)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Among users from different continents, which continent has the highest average 'totalHits'?"
      ],
      "metadata": {
        "id": "mVbSCSIDyDti"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "average_hits_by_continent = train_data.groupby('geoNetwork.continent')['totalHits'].mean()\n",
        "\n",
        "# Find the continent with the highest average totalHits\n",
        "continent_highest_hits = average_hits_by_continent.idxmax()\n",
        "highest_average_hits = average_hits_by_continent.max()\n",
        "\n",
        "print(f\"The continent with the highest average 'totalHits' is: {continent_highest_hits}\")"
      ],
      "metadata": {
        "id": "M28OXRY3yH9o",
        "outputId": "8e45536d-442e-4cad-c6a7-3dbb2b152373",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The continent with the highest average 'totalHits' is: Americas\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For users who made a purchase, what is the most common traffic source (medium)?"
      ],
      "metadata": {
        "id": "XSN4l1lIy11y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter data for rows where purchaseValue is not 0\n",
        "purchased_data = train_data[train_data['purchaseValue'] != 0]\n",
        "# Find the most frequent traffic source medium in the filtered data\n",
        "most_frequent_medium = purchased_data['trafficSource.medium'].mode()[0]\n",
        "print(f\"The most frequent trafficSource.medium where purchase value is not 0 is: {most_frequent_medium}\")"
      ],
      "metadata": {
        "id": "IFwUSy41y2xM",
        "outputId": "650fab1c-dfa4-44ca-e951-138364e9bee9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The most frequent trafficSource.medium where purchase value is not 0 is: (none)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Which combination of operating system and browser is most common among users who made a purchase?"
      ],
      "metadata": {
        "id": "j-ZPhelJ223p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Filter data for rows where purchaseValue is not 0\n",
        "purchased_data = train_data[train_data['purchaseValue'] != 0].copy()\n",
        "\n",
        "# Combine 'device.deviceCategory' and 'device.browser' into a new column\n",
        "purchased_data['device_browser_combination'] = purchased_data['os'] + ' - ' + purchased_data['browser']\n",
        "\n",
        "# Find the most frequent combination\n",
        "most_common_combination = purchased_data['device_browser_combination'].mode()[0]\n",
        "\n",
        "print(f\"The most common combination of deviceType and browser among users who made a purchase is: {most_common_combination}\")"
      ],
      "metadata": {
        "id": "vCfe8K4L3GCS",
        "outputId": "63ed07f6-f145-455b-c401-c61d3c78b737",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The most common combination of deviceType and browser among users who made a purchase is: Macintosh - Chrome\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is the ratio of average number of hits for sessions that resulted in a purchase compared to those that did not?"
      ],
      "metadata": {
        "id": "wwvHDXIo2Kr_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úÖ Ensure correct dtype\n",
        "train_data[\"totalHits\"] = pd.to_numeric(train_data[\"totalHits\"], errors=\"coerce\")\n",
        "train_data[\"purchaseValue\"] = pd.to_numeric(train_data[\"purchaseValue\"], errors=\"coerce\")\n",
        "\n",
        "# üßÆ Split into two groups\n",
        "hits_with_purchase = train_data[train_data[\"purchaseValue\"] > 0][\"totalHits\"]\n",
        "hits_without_purchase = train_data[train_data[\"purchaseValue\"] == 0][\"totalHits\"]\n",
        "\n",
        "# üìä Compute averages\n",
        "avg_hits_with = hits_with_purchase.mean()\n",
        "avg_hits_without = hits_without_purchase.mean()\n",
        "\n",
        "# üîÅ Compute ratio\n",
        "ratio = avg_hits_with / avg_hits_without\n",
        "\n",
        "# üñ®Ô∏è Display\n",
        "print(f\"‚úÖ Average hits WITH purchase     : {avg_hits_with:.2f}\")\n",
        "print(f\"‚úÖ Average hits WITHOUT purchase  : {avg_hits_without:.2f}\")\n",
        "print(f\"üîÅ Ratio (with / without)         : {ratio:.2f}\")\n"
      ],
      "metadata": {
        "id": "q1Sy13L42WaJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ceba786-7889-48db-a5fb-06279b0dcdbf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Average hits WITH purchase     : 35.53\n",
            "‚úÖ Average hits WITHOUT purchase  : 4.14\n",
            "üîÅ Ratio (with / without)         : 8.58\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is the proportion of 'organic' sessions in comparison to all other types of sessions from each traffic source medium?"
      ],
      "metadata": {
        "id": "kJHnTTpP5l5c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "organic_sessions = (train_data[\"trafficSource.medium\"] == \"organic\").sum()\n",
        "total_sessions = len(train_data)\n",
        "organic_proportion = organic_sessions / total_sessions\n",
        "print(f\"{organic_proportion:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G9VwjMcp5tZ8",
        "outputId": "4ad3b433-7af0-41be-c656-990a9b8e4eba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.35\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q7: Which referral path is most common for sessions that resulted in a purchase?"
      ],
      "metadata": {
        "id": "kW_xwhAa58cb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter users who made a purchase\n",
        "purchased = train_data[train_data[\"purchaseValue\"] > 0]\n",
        "\n",
        "# Most common referral path among purchasers\n",
        "most_common_referral = purchased[\"trafficSource.referralPath\"].value_counts().idxmax()\n",
        "count = purchased[\"trafficSource.referralPath\"].value_counts().max()\n",
        "\n",
        "print(f\"üîó Most common referral path for purchasers: '{most_common_referral}'\")\n",
        "print(f\"üìä Number of sessions: {count}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-s4QBT556BCy",
        "outputId": "2e59517e-1ae6-4bcd-8179-5560cadbcb64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîó Most common referral path for purchasers: '/'\n",
            "üìä Number of sessions: 5044\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q8: Which operating system has the highest average purchase value?"
      ],
      "metadata": {
        "id": "Pl70YS676OEb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Group by OS and calculate average purchase value\n",
        "os_avg_purchase = train_data.groupby(\"os\")[\"purchaseValue\"].mean().sort_values(ascending=False)\n",
        "\n",
        "# Get top OS\n",
        "top_os = os_avg_purchase.idxmax()\n",
        "top_value = os_avg_purchase.max()\n",
        "\n",
        "print(\"üíª OS with highest avg. purchaseValue:\", top_os)\n",
        "print(\"üí∞ Average purchaseValue:\", round(top_value, 2))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F0GwRMYM6SgK",
        "outputId": "dedb749c-9c41-4f15-f2cf-dcdec76f11ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üíª OS with highest avg. purchaseValue: Chrome OS\n",
            "üí∞ Average purchaseValue: 81268980.86\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q9: What is the average number of sessions per user?"
      ],
      "metadata": {
        "id": "7XdFdfCF6ck3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert sessionNumber to numeric if needed\n",
        "train_data[\"sessionNumber\"] = pd.to_numeric(train_data[\"sessionNumber\"], errors=\"coerce\")\n",
        "\n",
        "# Group by userId and calculate average sessions\n",
        "avg_sessions_per_user = train_data.groupby(\"userId\")[\"sessionNumber\"].mean().mean()\n",
        "\n",
        "print(f\"üìä Average number of sessions per user: {avg_sessions_per_user:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3uPBXlne6ihz",
        "outputId": "6678dd74-886b-4f06-b243-23636bd28673"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìä Average number of sessions per user: 1.98\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q10: How many unique users are there in the dataset?"
      ],
      "metadata": {
        "id": "CiKS-V_E6upH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Count unique users\n",
        "unique_users = train_data[\"userId\"].nunique()\n",
        "\n",
        "print(f\"üôã Unique users in the dataset: {unique_users}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LlBv0snD6ypq",
        "outputId": "e0937970-0afd-4aa3-df00-5ac87a0a5455"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üôã Unique users in the dataset: 100499\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MODELS"
      ],
      "metadata": {
        "id": "AjKjzuVorgEY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# X = train_data.drop(columns='purchaseValue')\n",
        "# y = train_data['purchaseValue']\n",
        "# test_data = test_data\n",
        "\n",
        "# # Make object columns categorical in both train and test\n",
        "# object_columns = X.select_dtypes(include='object').columns\n",
        "# for col in object_columns:\n",
        "#     X[col] = X[col].astype('category')\n",
        "#     if col in test_data.columns:\n",
        "#         test_data[col] = test_data[col].astype('category')\n",
        "\n",
        "# X_train,X_test,y_train,y_test=train_test_split(X,y,random_state=42,test_size=0.2)\n",
        "# model = lgb.LGBMRegressor(n_estimators=1000, learning_rate=0.05, random_state=42)\n",
        "# model.fit(X_train, y_train)\n",
        "\n",
        "# # Make predictions\n",
        "# y_pred = model.predict(test_data)\n",
        "\n",
        "# # Create submission DataFrame\n",
        "# submission = pd.DataFrame({\n",
        "#     'id':range(0,test_data.shape[0]),\n",
        "#     'purchaseValue': y_pred\n",
        "# })\n",
        "\n",
        "# # Save to CSV\n",
        "# submission.to_csv('submission.csv', index=False)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-25T05:54:38.837745Z",
          "iopub.execute_input": "2025-05-25T05:54:38.838025Z",
          "iopub.status.idle": "2025-05-25T05:54:38.84331Z",
          "shell.execute_reply.started": "2025-05-25T05:54:38.838005Z",
          "shell.execute_reply": "2025-05-25T05:54:38.842143Z"
        },
        "id": "Zp7XW7vZrgEY"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.model_selection import train_test_split\n",
        "# from sklearn.preprocessing import StandardScaler\n",
        "# from sklearn.impute import SimpleImputer\n",
        "# from sklearn.preprocessing import OneHotEncoder\n",
        "# from sklearn.compose import ColumnTransformer\n",
        "# from sklearn.pipeline import Pipeline\n",
        "\n",
        "# # Split into features and target\n",
        "# X = train_data.drop(columns=['purchaseValue'])\n",
        "# y = train_data['purchaseValue']\n",
        "\n",
        "# # Handling missing values and encoding categorical columns\n",
        "# numeric_features = X.select_dtypes(include=['int64', 'float64']).columns\n",
        "# categorical_features = X.select_dtypes(include=['object']).columns\n",
        "\n",
        "# # Preprocessing pipelines\n",
        "# numeric_pipeline = Pipeline([\n",
        "#     ('imputer', SimpleImputer(strategy='mean')),  # Handle missing values\n",
        "#     ('scaler', StandardScaler())  # Standardize numerical data\n",
        "# ])\n",
        "\n",
        "# categorical_pipeline = Pipeline([\n",
        "#     ('imputer', SimpleImputer(strategy='most_frequent')),  # Handle missing values\n",
        "#     ('encoder', OneHotEncoder(handle_unknown='ignore'))  # One-hot encoding for categorical features\n",
        "# ])\n",
        "\n",
        "# # Apply transformations to columns\n",
        "# preprocessor = ColumnTransformer([\n",
        "#     ('num', numeric_pipeline, numeric_features),\n",
        "#     ('cat', categorical_pipeline, categorical_features)\n",
        "# ])\n",
        "\n",
        "# # Split data into training and validation sets\n",
        "# X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-25T05:54:38.844766Z",
          "iopub.execute_input": "2025-05-25T05:54:38.845105Z",
          "iopub.status.idle": "2025-05-25T05:54:38.87264Z",
          "shell.execute_reply.started": "2025-05-25T05:54:38.845059Z",
          "shell.execute_reply": "2025-05-25T05:54:38.871693Z"
        },
        "id": "D1BGrjKSrgEY"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.model_selection import RandomizedSearchCV\n",
        "# from sklearn.ensemble import RandomForestRegressor\n",
        "# from sklearn.pipeline import Pipeline\n",
        "# from sklearn.metrics import r2_score\n",
        "\n",
        "# # Define a smaller and more efficient hyperparameter space\n",
        "# param_distributions = {\n",
        "#     'regressor__n_estimators': [50, 100],\n",
        "#     'regressor__max_depth': [10, 20, None],\n",
        "#     'regressor__min_samples_split': [2, 5],\n",
        "# }\n",
        "\n",
        "# # Build the model pipeline again if needed\n",
        "# model_pipeline = Pipeline([\n",
        "#     ('preprocessor', preprocessor),  # Your earlier preprocessing pipeline\n",
        "#     ('regressor', RandomForestRegressor(random_state=42))\n",
        "# ])\n",
        "\n",
        "# # Setup RandomizedSearchCV\n",
        "# random_search = RandomizedSearchCV(\n",
        "#     model_pipeline,\n",
        "#     param_distributions=param_distributions,\n",
        "#     n_iter=6,                # Try 6 random combinations (faster)\n",
        "#     cv=3,                    # Use 3-fold cross-validation\n",
        "#     scoring='neg_mean_squared_error',\n",
        "#     n_jobs=-1,               # Use all available cores\n",
        "#     verbose=1,\n",
        "#     random_state=42\n",
        "# )\n",
        "\n",
        "# # Fit on a subset (optional but can improve speed during testing)\n",
        "# X_sample = X_train\n",
        "# y_sample = y_train.loc[X_sample.index]\n",
        "\n",
        "# # Fit the model\n",
        "# random_search.fit(X_sample, y_sample)\n",
        "\n",
        "# # Best model\n",
        "# best_model = random_search.best_estimator_\n",
        "\n",
        "# # Evaluate on full validation set\n",
        "# y_pred_best = best_model.predict(X_val)\n",
        "# r2_best = r2_score(y_val,y_pred_best)\n",
        "# print(f'Best R¬≤ Score: {r2_best:.4f}')\n",
        "\n",
        "# # Create submission DataFrame\n",
        "# submission = pd.DataFrame({\n",
        "#     'id':range(0,test_data.shape[0]),\n",
        "#     'purchaseValue': y_pred_best\n",
        "# })\n",
        "\n",
        "# # Save to CSV\n",
        "# submission.to_csv('submission.csv', index=False)\n",
        "\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-25T05:54:38.873602Z",
          "iopub.execute_input": "2025-05-25T05:54:38.873876Z",
          "iopub.status.idle": "2025-05-25T05:54:38.896325Z",
          "shell.execute_reply.started": "2025-05-25T05:54:38.873853Z",
          "shell.execute_reply": "2025-05-25T05:54:38.895314Z"
        },
        "id": "oSIiaOFJrgEY"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "# from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "# from sklearn.compose import ColumnTransformer\n",
        "# from sklearn.pipeline import Pipeline\n",
        "# from sklearn.impute import SimpleImputer\n",
        "# from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "# from sklearn.metrics import r2_score\n",
        "# import lightgbm as lgb\n",
        "\n",
        "# # -------------------------------\n",
        "# # 1. Load your dataset\n",
        "# # -------------------------------\n",
        "# # Assume you already have train_data and test_data loaded\n",
        "# # train_data = pd.read_csv('train.csv')\n",
        "# # test_data = pd.read_csv('test.csv')\n",
        "\n",
        "# # -------------------------------\n",
        "# # 2. Prepare features and target\n",
        "# # -------------------------------\n",
        "# X = train_data.drop(columns=['purchaseValue'])\n",
        "# y = train_data['purchaseValue']\n",
        "\n",
        "# # Optional: log1p transform to handle skewed targets\n",
        "# y_log = np.log1p(y)\n",
        "\n",
        "# X_train, X_val, y_train, y_val = train_test_split(X, y_log, test_size=0.2, random_state=42)\n",
        "\n",
        "# # -------------------------------\n",
        "# # 3. Build preprocessing pipeline\n",
        "# # -------------------------------\n",
        "# numeric_features = X.select_dtypes(include=['int64', 'float64']).columns\n",
        "# categorical_features = X.select_dtypes(include=['object', 'category']).columns\n",
        "\n",
        "# numeric_transformer = Pipeline([\n",
        "#     ('imputer', SimpleImputer(strategy='mean')),\n",
        "#     ('scaler', StandardScaler())\n",
        "# ])\n",
        "\n",
        "# categorical_transformer = Pipeline([\n",
        "#     ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "#     ('encoder', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
        "# ])\n",
        "\n",
        "# preprocessor = ColumnTransformer([\n",
        "#     ('num', numeric_transformer, numeric_features),\n",
        "#     ('cat', categorical_transformer, categorical_features)\n",
        "# ])\n",
        "\n",
        "# # -------------------------------\n",
        "# # 4. Define the LightGBM model\n",
        "# # -------------------------------\n",
        "# lgb_model = lgb.LGBMRegressor(random_state=42)\n",
        "\n",
        "# # -------------------------------\n",
        "# # 5. Create the pipeline\n",
        "# # -------------------------------\n",
        "# pipeline = Pipeline([\n",
        "#     ('preprocessor', preprocessor),\n",
        "#     ('regressor', lgb_model)\n",
        "# ])\n",
        "\n",
        "# # -------------------------------\n",
        "# # 6. Hyperparameter tuning\n",
        "# # -------------------------------\n",
        "# param_distributions = {\n",
        "#     'regressor__n_estimators': [100, 200, 300],\n",
        "#     'regressor__learning_rate': [0.01, 0.05, 0.1],\n",
        "#     'regressor__max_depth': [5, 10, -1],\n",
        "#     'regressor__num_leaves': [15, 31, 63]\n",
        "# }\n",
        "\n",
        "# search = RandomizedSearchCV(\n",
        "#     pipeline,\n",
        "#     param_distributions=param_distributions,\n",
        "#     n_iter=10,\n",
        "#     scoring='r2',\n",
        "#     cv=3,\n",
        "#     verbose=1,\n",
        "#     n_jobs=-1,\n",
        "#     random_state=42\n",
        "# )\n",
        "\n",
        "# search.fit(X_train, y_train)\n",
        "\n",
        "# # -------------------------------\n",
        "# # 7. Evaluate\n",
        "# # -------------------------------\n",
        "# best_model = search.best_estimator_\n",
        "# y_val_pred_log = best_model.predict(X_val)\n",
        "# y_val_pred = np.expm1(y_val_pred_log)  # Inverse log1p\n",
        "# y_val_true = np.expm1(y_val)           # Inverse log1p\n",
        "\n",
        "# r2 = r2_score(y_val_true, y_val_pred)\n",
        "# print(f'Validation R¬≤ score: {r2:.4f}')\n",
        "\n",
        "# # -------------------------------\n",
        "# # 8. Predict on test set\n",
        "# # -------------------------------\n",
        "# X_test = test_data\n",
        "# test_preds_log = best_model.predict(X_test)\n",
        "# #test_preds = np.expm1(test_preds_log)\n",
        "\n",
        "# submission = pd.DataFrame({\n",
        "#     'id':range(0,test_data.shape[0]),\n",
        "#     'purchaseValue': test_preds_log\n",
        "# })\n",
        "# submission.to_csv('submission.csv', index=False)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-25T05:54:38.897364Z",
          "iopub.execute_input": "2025-05-25T05:54:38.897611Z",
          "iopub.status.idle": "2025-05-25T05:54:38.924602Z",
          "shell.execute_reply.started": "2025-05-25T05:54:38.897592Z",
          "shell.execute_reply": "2025-05-25T05:54:38.923056Z"
        },
        "id": "JNcgV4xxrgEZ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "# from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "# from sklearn.compose import ColumnTransformer\n",
        "# from sklearn.pipeline import Pipeline\n",
        "# from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "# from sklearn.impute import SimpleImputer\n",
        "# from sklearn.metrics import r2_score\n",
        "# import xgboost as xgb\n",
        "\n",
        "# # Load data\n",
        "# train_data = pd.read_csv(\"/kaggle/input/engage-2-value-from-clicks-to-conversions/train_data.csv\")\n",
        "\n",
        "# # Feature engineering: create a new feature combining device type and browser\n",
        "# def add_features(df):\n",
        "#     df['device_browser'] = df['deviceType'] + \"_\" + df['browser']\n",
        "#     return df\n",
        "\n",
        "# train_data = add_features(train_data)\n",
        "\n",
        "# # Drop columns that are clearly identifiers or too sparse for this context\n",
        "# drop_cols = ['userId', 'sessionId', 'date', 'sessionStart']\n",
        "# train_data = train_data.drop(columns=drop_cols, errors='ignore')\n",
        "\n",
        "# # Separate features and target\n",
        "# X = train_data.drop(columns=['purchaseValue'])\n",
        "# y = train_data['purchaseValue']\n",
        "\n",
        "# # Log transform to reduce skew\n",
        "# y_log = np.log1p(y)\n",
        "\n",
        "# # Split for validation\n",
        "# X_train, X_val, y_train, y_val = train_test_split(X, y_log, test_size=0.2, random_state=42)\n",
        "\n",
        "# # Column types\n",
        "# numeric_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "# categorical_features = X.select_dtypes(include=['object', 'bool']).columns.tolist()\n",
        "\n",
        "# # Pipelines for preprocessing\n",
        "# numeric_transformer = Pipeline([\n",
        "#     ('imputer', SimpleImputer(strategy='mean')),\n",
        "#     ('scaler', StandardScaler())\n",
        "# ])\n",
        "\n",
        "# categorical_transformer = Pipeline([\n",
        "#     ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "#     ('encoder', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
        "# ])\n",
        "\n",
        "# preprocessor = ColumnTransformer([\n",
        "#     ('num', numeric_transformer, numeric_features),\n",
        "#     ('cat', categorical_transformer, categorical_features)\n",
        "# ])\n",
        "\n",
        "# # XGBoost model\n",
        "# xgb_model = xgb.XGBRegressor(objective='reg:squarederror', random_state=42)\n",
        "\n",
        "# # Full pipeline\n",
        "# pipeline = Pipeline([\n",
        "#     ('preprocessor', preprocessor),\n",
        "#     ('regressor', xgb_model)\n",
        "# ])\n",
        "\n",
        "# # Basic hyperparameter tuning\n",
        "# param_distributions = {\n",
        "#     'regressor__n_estimators': [100, 200],\n",
        "#     'regressor__max_depth': [3, 5, 7],\n",
        "#     'regressor__learning_rate': [0.05, 0.1]\n",
        "# }\n",
        "\n",
        "# search = RandomizedSearchCV(\n",
        "#     pipeline,\n",
        "#     param_distributions=param_distributions,\n",
        "#     n_iter=6,\n",
        "#     scoring='r2',\n",
        "#     cv=3,\n",
        "#     verbose=1,\n",
        "#     random_state=42,\n",
        "#     n_jobs=-1\n",
        "# )\n",
        "\n",
        "# # Train the model\n",
        "# search.fit(X_train, y_train)\n",
        "\n",
        "# # Evaluate\n",
        "# best_model = search.best_estimator_\n",
        "# y_val_pred_log = best_model.predict(X_val)\n",
        "# y_val_pred = np.expm1(y_val_pred_log)\n",
        "# y_val_true = np.expm1(y_val)\n",
        "# r2 = r2_score(y_val_true, y_val_pred)\n",
        "# print(f'Validation R¬≤ score: {r2:.4f}')\n",
        "\n",
        "# test_data = pd.read_csv(\"/kaggle/input/engage-2-value-from-clicks-to-conversions/test_data.csv\")\n",
        "# test_data = add_features(test_data)\n",
        "# test_data = test_data.drop(columns=drop_cols, errors='ignore')\n",
        "# test_preds_log = best_model.predict(test_data)\n",
        "# test_preds = np.expm1(test_preds_log)\n",
        "# submission = pd.DataFrame({'id':range(0,test_data.shape[0]), 'purchaseValue': test_preds})\n",
        "# submission.to_csv(\"submission.csv\", index=False)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-25T05:54:38.925875Z",
          "iopub.execute_input": "2025-05-25T05:54:38.926263Z",
          "iopub.status.idle": "2025-05-25T05:54:38.952298Z",
          "shell.execute_reply.started": "2025-05-25T05:54:38.926239Z",
          "shell.execute_reply": "2025-05-25T05:54:38.951168Z"
        },
        "id": "v6-RUxggrgEZ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# from sklearn.compose import ColumnTransformer\n",
        "# from sklearn.pipeline import Pipeline\n",
        "# from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "# from sklearn.impute import SimpleImputer\n",
        "# from sklearn.ensemble import RandomForestRegressor\n",
        "# from sklearn.metrics import r2_score\n",
        "# from xgboost import XGBRegressor\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# # -----------------------------\n",
        "# # Step 1: Load and prepare data\n",
        "# # -----------------------------\n",
        "# train_data = pd.read_csv(\"/kaggle/input/engage-2-value-from-clicks-to-conversions/train_data.csv\")\n",
        "\n",
        "# # Drop ID/session/timestamp columns\n",
        "# drop_cols = [\"userId\", \"sessionId\", \"date\", \"sessionStart\"]\n",
        "# X = train_data.drop(columns=[\"purchaseValue\"] + drop_cols, errors='ignore')\n",
        "# y = train_data[\"purchaseValue\"]\n",
        "# y_log = np.log1p(y)\n",
        "\n",
        "# # Split data\n",
        "# X_train, X_val, y_train, y_val = train_test_split(X, y_log, test_size=0.2, random_state=42)\n",
        "\n",
        "# # Identify feature types\n",
        "# numeric_features = X.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\n",
        "# categorical_features = X.select_dtypes(include=[\"object\", \"bool\"]).columns.tolist()\n",
        "\n",
        "# print(f\"üìä Original features: {len(numeric_features)} numeric, {len(categorical_features)} categorical\")\n",
        "\n",
        "# # -----------------------------\n",
        "# # Step 2: Preprocessing\n",
        "# # -----------------------------\n",
        "# numeric_transformer = Pipeline([\n",
        "#     (\"imputer\", SimpleImputer(strategy=\"mean\")),\n",
        "#     (\"scaler\", StandardScaler())\n",
        "# ])\n",
        "\n",
        "# categorical_transformer = Pipeline([\n",
        "#     (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "#     (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))\n",
        "# ])\n",
        "\n",
        "# preprocessor = ColumnTransformer([\n",
        "#     (\"num\", numeric_transformer, numeric_features),\n",
        "#     (\"cat\", categorical_transformer, categorical_features)\n",
        "# ])\n",
        "\n",
        "# # -----------------------------\n",
        "# # Step 3: RandomForest for Feature Importance\n",
        "# # -----------------------------\n",
        "# rf_pipeline = Pipeline([\n",
        "#     (\"preprocessor\", preprocessor),\n",
        "#     (\"regressor\", RandomForestRegressor(random_state=42))\n",
        "# ])\n",
        "\n",
        "# rf_pipeline.fit(X_train, y_train)\n",
        "\n",
        "# # Get top features\n",
        "# feature_names = rf_pipeline.named_steps[\"preprocessor\"].get_feature_names_out()\n",
        "# importances = rf_pipeline.named_steps[\"regressor\"].feature_importances_\n",
        "\n",
        "# feat_df = pd.DataFrame({\"feature\": feature_names, \"importance\": importances})\n",
        "# feat_df = feat_df.sort_values(by=\"importance\", ascending=False)\n",
        "\n",
        "# print(f\"üîç Total transformed features: {len(feature_names)}\")\n",
        "# print(f\"üìà Max importance: {feat_df['importance'].max():.6f}\")\n",
        "# print(f\"üìâ Min importance: {feat_df['importance'].min():.6f}\")\n",
        "\n",
        "# # Keep features with importance > 0.001\n",
        "# important_features = feat_df[feat_df[\"importance\"] > 0.001][\"feature\"].tolist()\n",
        "# print(f\"üéØ Features with importance > 0.001: {len(important_features)}\")\n",
        "\n",
        "# # Extract original feature names from transformed feature names\n",
        "# original_feature_names = []\n",
        "# for feat in important_features:\n",
        "#     # Handle both numeric and categorical transformed names\n",
        "#     if \"__\" in feat:\n",
        "#         original_name = feat.split(\"__\")[1]  # For categorical: \"cat__feature_name\"\n",
        "#     else:\n",
        "#         original_name = feat.replace(\"num__\", \"\")  # For numeric: \"num__feature_name\"\n",
        "\n",
        "#     if original_name not in original_feature_names:\n",
        "#         original_feature_names.append(original_name)\n",
        "\n",
        "# print(f\"üîß Mapped to {len(original_feature_names)} original features\")\n",
        "\n",
        "# # üõ°Ô∏è Enhanced Fallback: If no features remain or mapping failed, keep all\n",
        "# if len(original_feature_names) == 0:\n",
        "#     print(\"‚ö†Ô∏è No features passed importance threshold or mapping failed. Using all features.\")\n",
        "#     original_feature_names = numeric_features + categorical_features\n",
        "\n",
        "# # Ensure we have valid features\n",
        "# reduced_numeric = [f for f in original_feature_names if f in numeric_features]\n",
        "# reduced_categorical = [f for f in original_feature_names if f in categorical_features]\n",
        "\n",
        "# print(f\"‚úÖ Final selection: {len(reduced_numeric)} numeric, {len(reduced_categorical)} categorical\")\n",
        "\n",
        "# # Additional safety check\n",
        "# if len(reduced_numeric) == 0 and len(reduced_categorical) == 0:\n",
        "#     print(\"üö® Emergency fallback: Using all original features\")\n",
        "#     reduced_numeric = numeric_features\n",
        "#     reduced_categorical = categorical_features\n",
        "\n",
        "# # -----------------------------\n",
        "# # Step 4: XGBoost Pipeline\n",
        "# # -----------------------------\n",
        "# reduced_preprocessor = ColumnTransformer([\n",
        "#     (\"num\", numeric_transformer, reduced_numeric),\n",
        "#     (\"cat\", categorical_transformer, reduced_categorical)\n",
        "# ])\n",
        "\n",
        "# xgb_pipeline = Pipeline([\n",
        "#     (\"preprocessor\", reduced_preprocessor),\n",
        "#     (\"regressor\", XGBRegressor(objective=\"reg:squarederror\", random_state=42))\n",
        "# ])\n",
        "\n",
        "# # Debug: Check preprocessor output shape\n",
        "# print(\"üî¨ Testing preprocessor...\")\n",
        "# X_train_transformed = reduced_preprocessor.fit_transform(X_train)\n",
        "# print(f\"üìê Transformed training data shape: {X_train_transformed.shape}\")\n",
        "\n",
        "# if X_train_transformed.shape[1] == 0:\n",
        "#     raise ValueError(\"‚ùå Preprocessor produced 0 features! Check feature selection logic.\")\n",
        "\n",
        "# # Fit model\n",
        "# print(\"üöÄ Training XGBoost model...\")\n",
        "# xgb_pipeline.fit(X_train, y_train)\n",
        "\n",
        "# # -----------------------------\n",
        "# # Step 5: Evaluate\n",
        "# # -----------------------------\n",
        "# y_val_pred_log = xgb_pipeline.predict(X_val)\n",
        "# y_val_pred = np.expm1(y_val_pred_log)\n",
        "# y_val_true = np.expm1(y_val)\n",
        "# r2 = r2_score(y_val_true, y_val_pred)\n",
        "# print(f\"‚úÖ Validation R¬≤ Score: {r2:.4f}\")\n",
        "\n",
        "# # -----------------------------\n",
        "# # Step 6: Predict on Test Set\n",
        "# # -----------------------------\n",
        "# test_data = pd.read_csv(\"/kaggle/input/engage-2-value-from-clicks-to-conversions/test_data.csv\")\n",
        "# test_data = test_data.drop(columns=drop_cols, errors='ignore')\n",
        "\n",
        "# # Predict\n",
        "# test_preds_log = xgb_pipeline.predict(test_data)\n",
        "# test_preds = np.expm1(test_preds_log)\n",
        "\n",
        "# # -----------------------------\n",
        "# # Step 7: Save submission.csv\n",
        "# # -----------------------------\n",
        "# submission = pd.DataFrame({\n",
        "#     \"id\": range(0, test_data.shape[0]),\n",
        "#     \"purchaseValue\": test_preds\n",
        "# })\n",
        "# submission.to_csv(\"submission.csv\", index=False)\n",
        "# print(\"üìÅ submission.csv saved successfully.\")\n",
        "\n",
        "# # Show top selected features for debugging\n",
        "# print(\"\\nüèÜ Top 10 selected features:\")\n",
        "# selected_features_df = feat_df[feat_df[\"feature\"].apply(\n",
        "#     lambda x: (x.split(\"__\")[1] if \"__\" in x else x.replace(\"num__\", \"\")) in original_feature_names\n",
        "# )].head(10)\n",
        "# for _, row in selected_features_df.iterrows():\n",
        "#     print(f\"  {row['feature']}: {row['importance']:.6f}\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-25T05:54:38.953627Z",
          "iopub.execute_input": "2025-05-25T05:54:38.953914Z",
          "iopub.status.idle": "2025-05-25T05:54:38.982766Z",
          "shell.execute_reply.started": "2025-05-25T05:54:38.953892Z",
          "shell.execute_reply": "2025-05-25T05:54:38.981141Z"
        },
        "id": "_L7fUgAArgEZ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "# from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
        "# from sklearn.compose import ColumnTransformer\n",
        "# from sklearn.pipeline import Pipeline\n",
        "# from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder, PolynomialFeatures\n",
        "# from sklearn.impute import SimpleImputer\n",
        "# from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "# from sklearn.linear_model import Ridge, ElasticNet\n",
        "# from sklearn.metrics import r2_score, mean_squared_error\n",
        "# from xgboost import XGBRegressor\n",
        "# from lightgbm import LGBMRegressor\n",
        "# import matplotlib.pyplot as plt\n",
        "# import seaborn as sns\n",
        "# import warnings\n",
        "# warnings.filterwarnings('ignore')\n",
        "\n",
        "# # -----------------------------\n",
        "# # Step 1: Load and Advanced EDA\n",
        "# # -----------------------------\n",
        "# print(\"üîÑ Loading and exploring data...\")\n",
        "# train_data = pd.read_csv(\"/kaggle/input/engage-2-value-from-clicks-to-conversions/train_data.csv\")\n",
        "# test_data = pd.read_csv(\"/kaggle/input/engage-2-value-from-clicks-to-conversions/test_data.csv\")\n",
        "\n",
        "# print(f\"üìä Train shape: {train_data.shape}, Test shape: {test_data.shape}\")\n",
        "# print(f\"üí∞ Target stats - Mean: {train_data['purchaseValue'].mean():.2f}, Std: {train_data['purchaseValue'].std():.2f}\")\n",
        "# print(f\"üéØ Zero purchases: {(train_data['purchaseValue'] == 0).sum()} ({(train_data['purchaseValue'] == 0).mean()*100:.1f}%)\")\n",
        "\n",
        "# # -----------------------------\n",
        "# # Step 2: Advanced Feature Engineering\n",
        "# # -----------------------------\n",
        "# def advanced_feature_engineering(df, is_train=True):\n",
        "#     \"\"\"Create advanced features from raw data\"\"\"\n",
        "#     df_fe = df.copy()\n",
        "\n",
        "#     # First, standardize data types to prevent mixed type errors\n",
        "#     print(\"üîß Standardizing data types...\")\n",
        "\n",
        "#     # Convert all object and boolean columns to string first\n",
        "#     for col in df_fe.columns:\n",
        "#         if df_fe[col].dtype == 'object' or df_fe[col].dtype == 'bool':\n",
        "#             df_fe[col] = df_fe[col].astype(str)\n",
        "\n",
        "#     # Date/Time features if available\n",
        "#     if 'date' in df_fe.columns:\n",
        "#         try:\n",
        "#             df_fe['date'] = pd.to_datetime(df_fe['date'])\n",
        "#             df_fe['dayOfWeek'] = df_fe['date'].dt.dayofweek\n",
        "#             df_fe['hour'] = df_fe['date'].dt.hour\n",
        "#             df_fe['month'] = df_fe['date'].dt.month\n",
        "#             df_fe['isWeekend'] = (df_fe['dayOfWeek'] >= 5).astype(int)\n",
        "#             df_fe['isBusinessHour'] = ((df_fe['hour'] >= 9) & (df_fe['hour'] <= 17)).astype(int)\n",
        "#         except:\n",
        "#             print(\"‚ö†Ô∏è Could not parse date column\")\n",
        "\n",
        "#     if 'sessionStart' in df_fe.columns:\n",
        "#         try:\n",
        "#             df_fe['sessionStart'] = pd.to_datetime(df_fe['sessionStart'])\n",
        "#             df_fe['sessionHour'] = df_fe['sessionStart'].dt.hour\n",
        "#             df_fe['sessionDayOfWeek'] = df_fe['sessionStart'].dt.dayofweek\n",
        "#         except:\n",
        "#             print(\"‚ö†Ô∏è Could not parse sessionStart column\")\n",
        "\n",
        "#     # Get categorical columns (now all strings)\n",
        "#     categorical_cols = []\n",
        "#     for col in df_fe.columns:\n",
        "#         if df_fe[col].dtype == 'object' and col not in ['userId', 'sessionId', 'date', 'sessionStart']:\n",
        "#             categorical_cols.append(col)\n",
        "\n",
        "#     # Create interaction features for top categorical pairs (limited to prevent explosion)\n",
        "#     if len(categorical_cols) >= 2:\n",
        "#         interaction_count = 0\n",
        "#         for i, col1 in enumerate(categorical_cols[:3]):  # Limit to prevent explosion\n",
        "#             for col2 in categorical_cols[i+1:4]:\n",
        "#                 if col1 != col2 and interaction_count < 5:  # Limit interactions\n",
        "#                     df_fe[f'{col1}_{col2}_interaction'] = df_fe[col1].astype(str) + '_' + df_fe[col2].astype(str)\n",
        "#                     interaction_count += 1\n",
        "\n",
        "#     # Numerical feature engineering\n",
        "#     numeric_cols = []\n",
        "#     for col in df_fe.columns:\n",
        "#         if df_fe[col].dtype in ['int64', 'float64'] and col not in ['purchaseValue', 'userId', 'sessionId']:\n",
        "#             numeric_cols.append(col)\n",
        "\n",
        "#     # Create polynomial and interaction features for important numerics (limited)\n",
        "#     feature_count = 0\n",
        "#     for col in numeric_cols[:3]:  # Further limit to prevent too many features\n",
        "#         if col in df_fe.columns and feature_count < 10:\n",
        "#             try:\n",
        "#                 # Square and log transforms with error handling\n",
        "#                 df_fe[f'{col}_squared'] = df_fe[col] ** 2\n",
        "#                 df_fe[f'{col}_log1p'] = np.log1p(np.abs(df_fe[col]))\n",
        "#                 df_fe[f'{col}_sqrt'] = np.sqrt(np.abs(df_fe[col]))\n",
        "#                 feature_count += 3\n",
        "#             except:\n",
        "#                 print(f\"‚ö†Ô∏è Could not create polynomial features for {col}\")\n",
        "\n",
        "#     # Binning continuous variables (limited)\n",
        "#     for col in numeric_cols[:2]:  # Limit binning\n",
        "#         if col in df_fe.columns and df_fe[col].nunique() > 10:\n",
        "#             try:\n",
        "#                 df_fe[f'{col}_binned'] = pd.qcut(df_fe[col], q=5, labels=False, duplicates='drop')\n",
        "#             except:\n",
        "#                 try:\n",
        "#                     df_fe[f'{col}_binned'] = pd.cut(df_fe[col], bins=5, labels=False, duplicates='drop')\n",
        "#                 except:\n",
        "#                     print(f\"‚ö†Ô∏è Could not create bins for {col}\")\n",
        "\n",
        "#     return df_fe\n",
        "\n",
        "# print(\"üîß Performing advanced feature engineering...\")\n",
        "# train_fe = advanced_feature_engineering(train_data, is_train=True)\n",
        "# test_fe = advanced_feature_engineering(test_data, is_train=False)\n",
        "\n",
        "# # -----------------------------\n",
        "# # Step 3: Smart Target Transformation\n",
        "# # -----------------------------\n",
        "# # Analyze target distribution\n",
        "# plt.figure(figsize=(12, 4))\n",
        "# plt.subplot(1, 3, 1)\n",
        "# plt.hist(train_data['purchaseValue'], bins=50, alpha=0.7)\n",
        "# plt.title('Original Target')\n",
        "# plt.subplot(1, 3, 2)\n",
        "# plt.hist(np.log1p(train_data['purchaseValue']), bins=50, alpha=0.7)\n",
        "# plt.title('Log1p Target')\n",
        "# plt.subplot(1, 3, 3)\n",
        "# plt.hist(np.sqrt(train_data['purchaseValue']), bins=50, alpha=0.7)\n",
        "# plt.title('Sqrt Target')\n",
        "# plt.tight_layout()\n",
        "# plt.show()\n",
        "\n",
        "# # Use Box-Cox-like transformation\n",
        "# y_original = train_data[\"purchaseValue\"]\n",
        "\n",
        "# # Try different transformations and pick the best\n",
        "# transformations = {\n",
        "#     'log1p': np.log1p(y_original),\n",
        "#     'sqrt': np.sqrt(y_original),\n",
        "#     'cbrt': np.cbrt(y_original),\n",
        "#     'log1p_plus_sqrt': np.log1p(y_original) + 0.1 * np.sqrt(y_original)\n",
        "# }\n",
        "\n",
        "# # Choose log1p as it typically works well for purchase data\n",
        "# y_transformed = transformations['log1p']\n",
        "# print(f\"üéØ Using log1p transformation for target\")\n",
        "\n",
        "# # -----------------------------\n",
        "# # Step 4: Intelligent Preprocessing\n",
        "# # -----------------------------\n",
        "# # Drop ID/session/timestamp columns\n",
        "# drop_cols = [\"userId\", \"sessionId\", \"date\", \"sessionStart\"]\n",
        "# X = train_fe.drop(columns=[\"purchaseValue\"] + drop_cols, errors='ignore')\n",
        "# y = y_transformed\n",
        "\n",
        "# # Ensure test set has same columns (except target)\n",
        "# test_X = test_fe.drop(columns=drop_cols, errors='ignore')\n",
        "\n",
        "# # Align columns between train and test\n",
        "# common_cols = list(set(X.columns) & set(test_X.columns))\n",
        "# X = X[common_cols]\n",
        "# test_X = test_X[common_cols]\n",
        "\n",
        "# print(f\"üìä Final feature count: {len(common_cols)}\")\n",
        "\n",
        "# # Split data with stratification based on target ranges\n",
        "# # Create target bins for stratification\n",
        "# y_bins = pd.qcut(y, q=5, labels=False, duplicates='drop')\n",
        "# X_train, X_val, y_train, y_val = train_test_split(\n",
        "#     X, y, test_size=0.2, random_state=42, stratify=y_bins\n",
        "# )\n",
        "\n",
        "# # Identify feature types with proper type checking\n",
        "# numeric_features = []\n",
        "# categorical_features = []\n",
        "\n",
        "# for col in X.columns:\n",
        "#     if X[col].dtype in ['int64', 'float64']:\n",
        "#         numeric_features.append(col)\n",
        "#     elif X[col].dtype == 'object':\n",
        "#         categorical_features.append(col)\n",
        "#     else:\n",
        "#         # Convert any remaining problematic types to string\n",
        "#         X[col] = X[col].astype(str)\n",
        "#         test_X[col] = test_X[col].astype(str)\n",
        "#         categorical_features.append(col)\n",
        "\n",
        "# print(f\"üìà Features: {len(numeric_features)} numeric, {len(categorical_features)} categorical\")\n",
        "\n",
        "# # Additional data cleaning for categorical features\n",
        "# print(\"üßπ Cleaning categorical data...\")\n",
        "# for col in categorical_features:\n",
        "#     # Replace any remaining problematic values\n",
        "#     X[col] = X[col].fillna('missing').astype(str)\n",
        "#     test_X[col] = test_X[col].fillna('missing').astype(str)\n",
        "\n",
        "#     # Handle any remaining boolean-like strings\n",
        "#     X[col] = X[col].replace({'True': 'true', 'False': 'false'})\n",
        "#     test_X[col] = test_X[col].replace({'True': 'true', 'False': 'false'})\n",
        "\n",
        "# # -----------------------------\n",
        "# # Step 5: Advanced Preprocessing Pipeline\n",
        "# # -----------------------------\n",
        "# # Enhanced numeric preprocessing\n",
        "# numeric_transformer = Pipeline([\n",
        "#     (\"imputer\", SimpleImputer(strategy=\"median\")),  # Median is more robust\n",
        "#     (\"scaler\", StandardScaler())\n",
        "# ])\n",
        "\n",
        "# # Enhanced categorical preprocessing with better error handling\n",
        "# def get_categorical_transformer():\n",
        "#     return Pipeline([\n",
        "#         (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=\"missing\")),\n",
        "#         (\"encoder\", OneHotEncoder(\n",
        "#             handle_unknown=\"ignore\",\n",
        "#             sparse_output=False,\n",
        "#             max_categories=15,  # Reduce to prevent too many features\n",
        "#             drop='if_binary'    # Drop one category for binary features\n",
        "#         ))\n",
        "#     ])\n",
        "\n",
        "# categorical_transformer = get_categorical_transformer()\n",
        "\n",
        "# preprocessor = ColumnTransformer([\n",
        "#     (\"num\", numeric_transformer, numeric_features),\n",
        "#     (\"cat\", categorical_transformer, categorical_features)\n",
        "# ], remainder='drop')\n",
        "\n",
        "# # -----------------------------\n",
        "# # Step 6: Feature Selection with Multiple Methods\n",
        "# # -----------------------------\n",
        "# print(\"üîç Performing intelligent feature selection...\")\n",
        "\n",
        "# # First, fit RandomForest for feature importance\n",
        "# rf_temp = Pipeline([\n",
        "#     (\"preprocessor\", preprocessor),\n",
        "#     (\"regressor\", RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1))\n",
        "# ])\n",
        "# rf_temp.fit(X_train, y_train)\n",
        "\n",
        "# # Get feature importance\n",
        "# feature_names = rf_temp.named_steps[\"preprocessor\"].get_feature_names_out()\n",
        "# importances = rf_temp.named_steps[\"regressor\"].feature_importances_\n",
        "\n",
        "# # Create feature importance dataframe\n",
        "# feat_df = pd.DataFrame({\"feature\": feature_names, \"importance\": importances})\n",
        "# feat_df = feat_df.sort_values(by=\"importance\", ascending=False)\n",
        "\n",
        "# # Dynamic threshold based on cumulative importance\n",
        "# cumsum_importance = feat_df['importance'].cumsum()\n",
        "# threshold_idx = np.where(cumsum_importance >= 0.95)[0][0]  # Keep features that explain 95% of importance\n",
        "# dynamic_threshold = feat_df.iloc[threshold_idx]['importance']\n",
        "\n",
        "# print(f\"üìä Dynamic importance threshold: {dynamic_threshold:.6f}\")\n",
        "# print(f\"üéØ Keeping {threshold_idx + 1} features out of {len(feature_names)}\")\n",
        "\n",
        "# # Select important features\n",
        "# important_features = feat_df.head(threshold_idx + 1)[\"feature\"].tolist()\n",
        "\n",
        "# # Map back to original features\n",
        "# original_feature_names = []\n",
        "# for feat in important_features:\n",
        "#     if \"__\" in feat:\n",
        "#         original_name = feat.split(\"__\")[1]\n",
        "#     else:\n",
        "#         original_name = feat.replace(\"num__\", \"\")\n",
        "\n",
        "#     if original_name not in original_feature_names:\n",
        "#         original_feature_names.append(original_name)\n",
        "\n",
        "# # Safety check\n",
        "# if len(original_feature_names) == 0:\n",
        "#     print(\"‚ö†Ô∏è Using all features as fallback\")\n",
        "#     original_feature_names = numeric_features + categorical_features\n",
        "\n",
        "# reduced_numeric = [f for f in original_feature_names if f in numeric_features]\n",
        "# reduced_categorical = [f for f in original_feature_names if f in categorical_features]\n",
        "\n",
        "# print(f\"‚úÖ Selected: {len(reduced_numeric)} numeric, {len(reduced_categorical)} categorical\")\n",
        "\n",
        "# # -----------------------------\n",
        "# # Step 7: Advanced Model Ensemble\n",
        "# # -----------------------------\n",
        "# # Create reduced preprocessor\n",
        "# reduced_preprocessor = ColumnTransformer([\n",
        "#     (\"num\", numeric_transformer, reduced_numeric),\n",
        "#     (\"cat\", categorical_transformer, reduced_categorical)\n",
        "# ])\n",
        "\n",
        "# # Define multiple models for ensemble\n",
        "# models = {\n",
        "#     'xgb': XGBRegressor(\n",
        "#         objective=\"reg:squarederror\",\n",
        "#         n_estimators=500,\n",
        "#         max_depth=6,\n",
        "#         learning_rate=0.05,\n",
        "#         subsample=0.8,\n",
        "#         colsample_bytree=0.8,\n",
        "#         random_state=42,\n",
        "#         n_jobs=-1\n",
        "#     ),\n",
        "#     'lgb': LGBMRegressor(\n",
        "#         objective='regression',\n",
        "#         n_estimators=500,\n",
        "#         max_depth=6,\n",
        "#         learning_rate=0.05,\n",
        "#         subsample=0.8,\n",
        "#         colsample_bytree=0.8,\n",
        "#         random_state=42,\n",
        "#         n_jobs=-1,\n",
        "#         verbose=-1\n",
        "#     ),\n",
        "#     'rf': RandomForestRegressor(\n",
        "#         n_estimators=300,\n",
        "#         max_depth=10,\n",
        "#         min_samples_split=5,\n",
        "#         min_samples_leaf=2,\n",
        "#         random_state=42,\n",
        "#         n_jobs=-1\n",
        "#     ),\n",
        "#     'gb': GradientBoostingRegressor(\n",
        "#         n_estimators=300,\n",
        "#         max_depth=6,\n",
        "#         learning_rate=0.05,\n",
        "#         subsample=0.8,\n",
        "#         random_state=42\n",
        "#     )\n",
        "# }\n",
        "\n",
        "# # Train individual models and collect predictions\n",
        "# print(\"üöÄ Training ensemble models...\")\n",
        "# model_predictions = {}\n",
        "# model_scores = {}\n",
        "\n",
        "# for name, model in models.items():\n",
        "#     print(f\"   Training {name.upper()}...\")\n",
        "\n",
        "#     pipeline = Pipeline([\n",
        "#         (\"preprocessor\", reduced_preprocessor),\n",
        "#         (\"regressor\", model)\n",
        "#     ])\n",
        "\n",
        "#     # Fit model\n",
        "#     pipeline.fit(X_train, y_train)\n",
        "\n",
        "#     # Predict on validation\n",
        "#     y_val_pred = pipeline.predict(X_val)\n",
        "#     r2 = r2_score(y_val, y_val_pred)\n",
        "\n",
        "#     model_predictions[name] = y_val_pred\n",
        "#     model_scores[name] = r2\n",
        "\n",
        "#     print(f\"   {name.upper()} R¬≤ Score: {r2:.4f}\")\n",
        "\n",
        "# # -----------------------------\n",
        "# # Step 8: Optimal Ensemble Weighting\n",
        "# # -----------------------------\n",
        "# print(\"üéØ Finding optimal ensemble weights...\")\n",
        "\n",
        "# # Simple average ensemble\n",
        "# ensemble_pred = np.mean([pred for pred in model_predictions.values()], axis=0)\n",
        "# ensemble_r2 = r2_score(y_val, ensemble_pred)\n",
        "# print(f\"üìä Simple Average Ensemble R¬≤ Score: {ensemble_r2:.4f}\")\n",
        "\n",
        "# # Weighted ensemble based on individual performance\n",
        "# weights = np.array([model_scores[name] for name in models.keys()])\n",
        "# weights = weights / weights.sum()  # Normalize\n",
        "\n",
        "# weighted_ensemble_pred = np.average(\n",
        "#     [model_predictions[name] for name in models.keys()],\n",
        "#     axis=0,\n",
        "#     weights=weights\n",
        "# )\n",
        "# weighted_ensemble_r2 = r2_score(y_val, weighted_ensemble_pred)\n",
        "# print(f\"‚öñÔ∏è Weighted Ensemble R¬≤ Score: {weighted_ensemble_r2:.4f}\")\n",
        "\n",
        "# # Choose best ensemble method\n",
        "# if weighted_ensemble_r2 > ensemble_r2:\n",
        "#     final_ensemble_pred = weighted_ensemble_pred\n",
        "#     final_r2 = weighted_ensemble_r2\n",
        "#     print(\"‚úÖ Using weighted ensemble\")\n",
        "# else:\n",
        "#     final_ensemble_pred = ensemble_pred\n",
        "#     final_r2 = ensemble_r2\n",
        "#     print(\"‚úÖ Using simple average ensemble\")\n",
        "\n",
        "# # -----------------------------\n",
        "# # Step 9: Final Model Training on Full Data\n",
        "# # -----------------------------\n",
        "# print(\"üèÅ Training final models on full training data...\")\n",
        "\n",
        "# # Retrain all models on full training data\n",
        "# final_models = {}\n",
        "# for name, model in models.items():\n",
        "#     pipeline = Pipeline([\n",
        "#         (\"preprocessor\", reduced_preprocessor),\n",
        "#         (\"regressor\", model)\n",
        "#     ])\n",
        "#     pipeline.fit(X, y)\n",
        "#     final_models[name] = pipeline\n",
        "\n",
        "# # -----------------------------\n",
        "# # Step 10: Test Predictions\n",
        "# # -----------------------------\n",
        "# print(\"üîÆ Generating test predictions...\")\n",
        "\n",
        "# # Generate predictions from all models\n",
        "# test_predictions = {}\n",
        "# for name, model in final_models.items():\n",
        "#     test_pred_log = model.predict(test_X)\n",
        "#     test_predictions[name] = test_pred_log\n",
        "\n",
        "# # Create ensemble prediction\n",
        "# if weighted_ensemble_r2 > ensemble_r2:\n",
        "#     final_test_pred_log = np.average(\n",
        "#         [test_predictions[name] for name in models.keys()],\n",
        "#         axis=0,\n",
        "#         weights=weights\n",
        "#     )\n",
        "# else:\n",
        "#     final_test_pred_log = np.mean([pred for pred in test_predictions.values()], axis=0)\n",
        "\n",
        "# # Transform back to original scale\n",
        "# final_test_pred = np.expm1(final_test_pred_log)\n",
        "\n",
        "# # Ensure no negative predictions\n",
        "# final_test_pred = np.maximum(final_test_pred, 0)\n",
        "\n",
        "# # -----------------------------\n",
        "# # Step 11: Create Submission\n",
        "# # -----------------------------\n",
        "# submission = pd.DataFrame({\n",
        "#     \"id\": range(len(final_test_pred)),\n",
        "#     \"purchaseValue\": final_test_pred\n",
        "# })\n",
        "\n",
        "# submission.to_csv(\"submission.csv\", index=False)\n",
        "# print(\"üìÅ submission.csv saved successfully!\")\n",
        "\n",
        "# # -----------------------------\n",
        "# # Step 12: Model Analysis and Insights\n",
        "# # -----------------------------\n",
        "# print(\"\\nüèÜ Model Performance Summary:\")\n",
        "# print(\"=\" * 50)\n",
        "# for name, score in model_scores.items():\n",
        "#     print(f\"{name.upper():>12}: {score:.4f}\")\n",
        "# print(f\"{'ENSEMBLE':>12}: {final_r2:.4f}\")\n",
        "# print(\"=\" * 50)\n",
        "\n",
        "# print(f\"\\nüìà Final Validation R¬≤ Score: {final_r2:.4f}\")\n",
        "# print(f\"üìä Prediction range: ${final_test_pred.min():.2f} - ${final_test_pred.max():.2f}\")\n",
        "# print(f\"üí∞ Mean predicted value: ${final_test_pred.mean():.2f}\")\n",
        "\n",
        "# # Show top features\n",
        "# print(f\"\\nüîç Top 15 Most Important Features:\")\n",
        "# print(\"-\" * 50)\n",
        "# for i, (_, row) in enumerate(feat_df.head(15).iterrows()):\n",
        "#     feature_name = row['feature']\n",
        "#     if \"__\" in feature_name:\n",
        "#         display_name = feature_name.split(\"__\")[1]\n",
        "#     else:\n",
        "#         display_name = feature_name.replace(\"num__\", \"\")\n",
        "#     print(f\"{i+1:>2}. {display_name:<30} ({row['importance']:.4f})\")\n",
        "\n",
        "# print(\"\\n‚úÖ Enhanced model training completed!\")\n",
        "# print(\"üéØ This model includes:\")\n",
        "# print(\"   ‚Ä¢ Advanced feature engineering\")\n",
        "# print(\"   ‚Ä¢ Multiple transformation strategies\")\n",
        "# print(\"   ‚Ä¢ Intelligent feature selection\")\n",
        "# print(\"   ‚Ä¢ Ensemble of 4 different algorithms\")\n",
        "# print(\"   ‚Ä¢ Optimal weight calculation\")\n",
        "# print(\"   ‚Ä¢ Robust preprocessing pipeline\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-25T05:54:38.984488Z",
          "iopub.execute_input": "2025-05-25T05:54:38.984804Z",
          "iopub.status.idle": "2025-05-25T05:54:39.016866Z",
          "shell.execute_reply.started": "2025-05-25T05:54:38.984783Z",
          "shell.execute_reply": "2025-05-25T05:54:39.014901Z"
        },
        "id": "vZRv7VbHrgEa"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.model_selection import train_test_split\n",
        "# from sklearn.preprocessing import StandardScaler\n",
        "# from sklearn.impute import SimpleImputer\n",
        "# from sklearn.preprocessing import OneHotEncoder\n",
        "# from sklearn.compose import ColumnTransformer\n",
        "# from sklearn.pipeline import Pipeline\n",
        "# from sklearn.ensemble import RandomForestRegressor\n",
        "# from sklearn.linear_model import Ridge\n",
        "# from sklearn.metrics import r2_score\n",
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "\n",
        "# # Load your data (adjust file paths as needed)\n",
        "# print(\"Loading data...\")\n",
        "# train_data = pd.read_csv('/kaggle/input/engage-2-value-from-clicks-to-conversions/train_data.csv')\n",
        "# test_data = pd.read_csv('/kaggle/input/engage-2-value-from-clicks-to-conversions/test_data.csv')\n",
        "# print(f\"Train data shape: {train_data.shape}\")\n",
        "# print(f\"Test data shape: {test_data.shape}\")\n",
        "# print(f\"Train columns: {list(train_data.columns)}\")\n",
        "# print(f\"Test columns: {list(test_data.columns)}\")\n",
        "\n",
        "# # Enhanced feature engineering function\n",
        "# def create_features(df):\n",
        "#     df = df.copy()\n",
        "\n",
        "#     # Age-based features\n",
        "#     if 'age' in df.columns:\n",
        "#         df['age_squared'] = df['age'] ** 2\n",
        "#         df['age_bins'] = pd.cut(df['age'], bins=[0, 25, 35, 50, 65, 100], labels=['young', 'adult', 'middle', 'senior', 'elderly'])\n",
        "\n",
        "#     # Income-based features\n",
        "#     if 'income' in df.columns:\n",
        "#         df['income_log'] = np.log1p(df['income'])  # log transform to handle skewness\n",
        "#         df['income_bins'] = pd.qcut(df['income'], q=5, labels=['low', 'below_avg', 'avg', 'above_avg', 'high'], duplicates='drop')\n",
        "\n",
        "#     # Interaction features (if both exist)\n",
        "#     if 'age' in df.columns and 'income' in df.columns:\n",
        "#         df['age_income_ratio'] = df['age'] / (df['income'] + 1)\n",
        "#         df['income_per_age'] = df['income'] / (df['age'] + 1)\n",
        "\n",
        "#     # Family size features\n",
        "#     if 'familySize' in df.columns:\n",
        "#         df['is_single'] = (df['familySize'] == 1).astype(int)\n",
        "#         df['large_family'] = (df['familySize'] >= 5).astype(int)\n",
        "\n",
        "#     return df\n",
        "\n",
        "# # Apply feature engineering\n",
        "# print(\"Creating enhanced features...\")\n",
        "# X_enhanced = create_features(train_data.drop(columns=['purchaseValue']))\n",
        "# y = train_data['purchaseValue']\n",
        "\n",
        "# # Identify feature types\n",
        "# numeric_features = X_enhanced.select_dtypes(include=['int64', 'float64']).columns\n",
        "# categorical_features = X_enhanced.select_dtypes(include=['object', 'category']).columns\n",
        "\n",
        "# print(f\"Numeric features: {len(numeric_features)}\")\n",
        "# print(f\"Categorical features: {len(categorical_features)}\")\n",
        "\n",
        "# # Preprocessing pipelines\n",
        "# numeric_pipeline = Pipeline([\n",
        "#     ('imputer', SimpleImputer(strategy='median')),  # Median is more robust to outliers\n",
        "#     ('scaler', StandardScaler())\n",
        "# ])\n",
        "\n",
        "# categorical_pipeline = Pipeline([\n",
        "#     ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "#     ('encoder', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
        "# ])\n",
        "\n",
        "# # Combine preprocessing\n",
        "# preprocessor = ColumnTransformer([\n",
        "#     ('num', numeric_pipeline, numeric_features),\n",
        "#     ('cat', categorical_pipeline, categorical_features)\n",
        "# ])\n",
        "\n",
        "# # Split data\n",
        "# X_train, X_val, y_train, y_val = train_test_split(X_enhanced, y, test_size=0.2, random_state=42, stratify=pd.qcut(y, q=5, duplicates='drop'))\n",
        "\n",
        "# print(\"Training models...\")\n",
        "\n",
        "# # Model 1: Optimized Random Forest (faster parameters)\n",
        "# rf_model = Pipeline([\n",
        "#     ('preprocessor', preprocessor),\n",
        "#     ('regressor', RandomForestRegressor(\n",
        "#         n_estimators=100,        # Reasonable number\n",
        "#         max_depth=15,           # Prevent overfitting\n",
        "#         min_samples_split=5,    # Faster training\n",
        "#         min_samples_leaf=2,     # Prevent overfitting\n",
        "#         max_features='sqrt',    # Faster training\n",
        "#         random_state=42,\n",
        "#         n_jobs=-1               # Use all cores\n",
        "#     ))\n",
        "# ])\n",
        "\n",
        "# # Model 2: Ridge Regression (very fast)\n",
        "# ridge_model = Pipeline([\n",
        "#     ('preprocessor', preprocessor),\n",
        "#     ('regressor', Ridge(alpha=1.0, random_state=42))\n",
        "# ])\n",
        "\n",
        "# # Train models\n",
        "# print(\"Training Random Forest...\")\n",
        "# rf_model.fit(X_train, y_train)\n",
        "\n",
        "# print(\"Training Ridge Regression...\")\n",
        "# ridge_model.fit(X_train, y_train)\n",
        "\n",
        "# # Evaluate models\n",
        "# rf_pred = rf_model.predict(X_val)\n",
        "# ridge_pred = ridge_model.predict(X_val)\n",
        "\n",
        "# rf_r2 = r2_score(y_val, rf_pred)\n",
        "# ridge_r2 = r2_score(y_val, ridge_pred)\n",
        "\n",
        "# print(f'Random Forest R¬≤ Score: {rf_r2:.4f}')\n",
        "# print(f'Ridge Regression R¬≤ Score: {ridge_r2:.4f}')\n",
        "\n",
        "# # Ensemble approach - simple average\n",
        "# ensemble_pred = (rf_pred + ridge_pred) / 2\n",
        "# ensemble_r2 = r2_score(y_val, ensemble_pred)\n",
        "# print(f'Ensemble R¬≤ Score: {ensemble_r2:.4f}')\n",
        "\n",
        "# # Choose best model\n",
        "# models = {'RF': (rf_model, rf_r2), 'Ridge': (ridge_model, ridge_r2)}\n",
        "# best_single_name, (best_single_model, best_single_r2) = max(models.items(), key=lambda x: x[1][1])\n",
        "\n",
        "# if ensemble_r2 > best_single_r2:\n",
        "#     print(f'Best model: Ensemble (R¬≤ = {ensemble_r2:.4f})')\n",
        "#     use_ensemble = True\n",
        "# else:\n",
        "#     print(f'Best model: {best_single_name} (R¬≤ = {best_single_r2:.4f})')\n",
        "#     use_ensemble = False\n",
        "\n",
        "# # Prepare test data\n",
        "# print(\"Preparing test predictions...\")\n",
        "# X_test_enhanced = create_features(test_data)\n",
        "\n",
        "# # Make predictions\n",
        "# if use_ensemble:\n",
        "#     rf_test_pred = rf_model.predict(X_test_enhanced)\n",
        "#     ridge_test_pred = ridge_model.predict(X_test_enhanced)\n",
        "#     test_predictions = (rf_test_pred + ridge_test_pred) / 2\n",
        "# else:\n",
        "#     test_predictions = best_single_model.predict(X_test_enhanced)\n",
        "\n",
        "# # Create submission\n",
        "# submission = pd.DataFrame({\n",
        "#     'id': range(0, test_data.shape[0]),\n",
        "#     'purchaseValue': test_predictions\n",
        "# })\n",
        "\n",
        "# submission.to_csv('submission.csv', index=False)\n",
        "# print(\"Submission file saved as 'submission.csv'\")\n",
        "\n",
        "# # Feature importance (if Random Forest is best)\n",
        "# if not use_ensemble and best_single_name == 'RF':\n",
        "#     feature_names = (list(numeric_features) +\n",
        "#                     list(rf_model.named_steps['preprocessor']\n",
        "#                          .named_transformers_['cat']\n",
        "#                          .named_steps['encoder']\n",
        "#                          .get_feature_names_out(categorical_features)))\n",
        "\n",
        "#     importances = rf_model.named_steps['regressor'].feature_importances_\n",
        "#     feature_importance = pd.DataFrame({\n",
        "#         'feature': feature_names,\n",
        "#         'importance': importances\n",
        "#     }).sort_values('importance', ascending=False)\n",
        "\n",
        "#     print(\"\\nTop 10 Most Important Features:\")\n",
        "#     print(feature_importance.head(10))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-28T23:30:28.761752Z",
          "iopub.execute_input": "2025-05-28T23:30:28.762127Z",
          "iopub.status.idle": "2025-05-28T23:31:39.267086Z",
          "shell.execute_reply.started": "2025-05-28T23:30:28.762099Z",
          "shell.execute_reply": "2025-05-28T23:31:39.265877Z"
        },
        "id": "aRQv2IpSrgEb"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# import numpy as np\n",
        "# import pandas as pd\n",
        "# from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "# from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "# from sklearn.impute import SimpleImputer\n",
        "# from sklearn.compose import ColumnTransformer\n",
        "# from sklearn.pipeline import Pipeline\n",
        "# from sklearn.metrics import r2_score\n",
        "# from lightgbm import LGBMRegressor\n",
        "\n",
        "# # =======================\n",
        "# # 1. Load the data\n",
        "# # =======================\n",
        "# train_data = pd.read_csv('/kaggle/input/engage-2-value-from-clicks-to-conversions/train_data.csv')\n",
        "# test_data = pd.read_csv('/kaggle/input/engage-2-value-from-clicks-to-conversions/test_data.csv')\n",
        "\n",
        "# # =======================\n",
        "# # 2. Feature/target split\n",
        "# # =======================\n",
        "# X = train_data.drop(columns=['purchaseValue'])\n",
        "# y = train_data['purchaseValue']\n",
        "\n",
        "# # =======================\n",
        "# # 3. Preprocessing\n",
        "# # =======================\n",
        "# numeric_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "# categorical_features = X.select_dtypes(include=['object']).columns.tolist()\n",
        "\n",
        "# numeric_pipeline = Pipeline([\n",
        "#     ('imputer', SimpleImputer(strategy='mean')),\n",
        "#     ('scaler', StandardScaler())\n",
        "# ])\n",
        "\n",
        "# categorical_pipeline = Pipeline([\n",
        "#     ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "#     ('encoder', OneHotEncoder(handle_unknown='ignore', sparse=False))\n",
        "# ])\n",
        "\n",
        "# preprocessor = ColumnTransformer([\n",
        "#     ('num', numeric_pipeline, numeric_features),\n",
        "#     ('cat', categorical_pipeline, categorical_features)\n",
        "# ])\n",
        "\n",
        "# # =======================\n",
        "# # 4. Train/validation split\n",
        "# # =======================\n",
        "# X_train, X_val, y_train, y_val = train_test_split(\n",
        "#     X, y, test_size=0.2, random_state=42\n",
        "# )\n",
        "\n",
        "# # =======================\n",
        "# # 5. Model pipeline\n",
        "# # =======================\n",
        "# model_pipeline = Pipeline([\n",
        "#     ('preprocessor', preprocessor),\n",
        "#     ('regressor', LGBMRegressor(random_state=42))\n",
        "# ])\n",
        "\n",
        "# # =======================\n",
        "# # 6. Hyperparameter tuning\n",
        "# # =======================\n",
        "# param_distributions = {\n",
        "#     'regressor__n_estimators': [100, 200, 300],\n",
        "#     'regressor__max_depth': [3, 6, 10],\n",
        "#     'regressor__learning_rate': [0.01, 0.1, 0.2],\n",
        "#     'regressor__subsample': [0.7, 0.8, 1.0],\n",
        "#     'regressor__colsample_bytree': [0.7, 0.8, 1.0]\n",
        "# }\n",
        "\n",
        "# random_search = RandomizedSearchCV(\n",
        "#     model_pipeline,\n",
        "#     param_distributions=param_distributions,\n",
        "#     n_iter=20,\n",
        "#     cv=3,\n",
        "#     scoring='r2',\n",
        "#     n_jobs=-1,\n",
        "#     verbose=1,\n",
        "#     random_state=42\n",
        "# )\n",
        "\n",
        "# random_search.fit(X_train, y_train)\n",
        "\n",
        "# # =======================\n",
        "# # 7. Validation\n",
        "# # =======================\n",
        "# best_model = random_search.best_estimator_\n",
        "# y_pred = best_model.predict(X_val)\n",
        "# r2 = r2_score(y_val, y_pred)\n",
        "# print(f'Validation R2 Score: {r2:.4f}')\n",
        "\n",
        "# # =======================\n",
        "# # 8. Test prediction & submission\n",
        "# # =======================\n",
        "# X_test = test_data  # Ensure test_data has the same columns as X\n",
        "# test_predictions = best_model.predict(X_test)\n",
        "\n",
        "# submission = pd.DataFrame({\n",
        "#     'id': range(len(test_predictions)),\n",
        "#     'purchaseValue': test_predictions\n",
        "# })\n",
        "# submission.to_csv('submission.csv', index=False)\n",
        "# print(\"Submission file 'submission.csv' created successfully.\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-30T14:21:49.017791Z",
          "iopub.execute_input": "2025-05-30T14:21:49.018118Z",
          "iopub.status.idle": "2025-05-30T14:23:32.125318Z",
          "shell.execute_reply.started": "2025-05-30T14:21:49.018086Z",
          "shell.execute_reply": "2025-05-30T14:23:32.123763Z"
        },
        "id": "PCCwHe0YrgEb"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# import numpy as np\n",
        "# import pandas as pd\n",
        "# from sklearn.model_selection import train_test_split, KFold, RandomizedSearchCV\n",
        "# from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "# from sklearn.impute import SimpleImputer\n",
        "# from sklearn.compose import ColumnTransformer\n",
        "# from sklearn.pipeline import Pipeline\n",
        "# from sklearn.metrics import r2_score\n",
        "# from lightgbm import LGBMRegressor\n",
        "\n",
        "# # =======================\n",
        "# # 1. Load the data from URLs\n",
        "# # =======================\n",
        "\n",
        "# train_data = pd.read_csv('/kaggle/input/engage-2-value-from-clicks-to-conversions/train_data.csv')\n",
        "# test_data = pd.read_csv('/kaggle/input/engage-2-value-from-clicks-to-conversions/test_data.csv')\n",
        "\n",
        "# # =======================\n",
        "# # 2. Feature Engineering (example: add more if you can)\n",
        "# # =======================\n",
        "# # Example: create interaction feature (customize based on your columns)\n",
        "# if 'device' in train_data.columns and 'browser' in train_data.columns:\n",
        "#     train_data['device_browser'] = train_data['device'] + '_' + train_data['browser']\n",
        "#     test_data['device_browser'] = test_data['device'] + '_' + test_data['browser']\n",
        "\n",
        "# # =======================\n",
        "# # 3. Prepare features and target\n",
        "# # =======================\n",
        "# X = train_data.drop(columns=['purchaseValue'])\n",
        "# y = train_data['purchaseValue']\n",
        "# X_test = test_data.copy()\n",
        "\n",
        "# # Optional: Log-transform target if highly skewed\n",
        "# # y = np.log1p(y)\n",
        "\n",
        "# # =======================\n",
        "# # 4. Preprocessing\n",
        "# # =======================\n",
        "# numeric_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "# categorical_features = X.select_dtypes(include=['object']).columns.tolist()\n",
        "\n",
        "# numeric_pipeline = Pipeline([\n",
        "#     ('imputer', SimpleImputer(strategy='mean')),\n",
        "#     ('scaler', StandardScaler())\n",
        "# ])\n",
        "\n",
        "# categorical_pipeline = Pipeline([\n",
        "#     ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "#     ('encoder', OneHotEncoder(handle_unknown='ignore', sparse=False))\n",
        "# ])\n",
        "\n",
        "# preprocessor = ColumnTransformer([\n",
        "#     ('num', numeric_pipeline, numeric_features),\n",
        "#     ('cat', categorical_pipeline, categorical_features)\n",
        "# ])\n",
        "\n",
        "# # =======================\n",
        "# # 5. Model and Hyperparameter Tuning\n",
        "# # =======================\n",
        "# model = Pipeline([\n",
        "#     ('preprocessor', preprocessor),\n",
        "#     ('regressor', LGBMRegressor(random_state=42))\n",
        "# ])\n",
        "\n",
        "# param_distributions = {\n",
        "#     'regressor__n_estimators': [200, 300, 500],\n",
        "#     'regressor__max_depth': [6, 10, 15],\n",
        "#     'regressor__learning_rate': [0.01, 0.05, 0.1],\n",
        "#     'regressor__num_leaves': [31, 50, 100],\n",
        "#     'regressor__reg_alpha': [0, 0.1, 1],\n",
        "#     'regressor__reg_lambda': [0, 0.1, 1],\n",
        "#     'regressor__subsample': [0.7, 0.8, 1.0],\n",
        "#     'regressor__colsample_bytree': [0.7, 0.8, 1.0]\n",
        "# }\n",
        "\n",
        "# # K-Fold Cross Validation\n",
        "# kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "# random_search = RandomizedSearchCV(\n",
        "#     model,\n",
        "#     param_distributions=param_distributions,\n",
        "#     n_iter=20,\n",
        "#     cv=kf,\n",
        "#     scoring='r2',\n",
        "#     n_jobs=-1,\n",
        "#     verbose=1,\n",
        "#     random_state=42\n",
        "# )\n",
        "\n",
        "# random_search.fit(X, y)\n",
        "# print(f\"Best R2 from CV: {random_search.best_score_:.4f}\")\n",
        "\n",
        "# # =======================\n",
        "# # 6. Predict on Test Set\n",
        "# # =======================\n",
        "# best_model = random_search.best_estimator_\n",
        "# test_predictions = best_model.predict(X_test)\n",
        "\n",
        "# # If you log-transformed y earlier, reverse it here:\n",
        "# # test_predictions = np.expm1(test_predictions)\n",
        "\n",
        "# # =======================\n",
        "# # 7. Prepare Submission\n",
        "# # =======================\n",
        "# # If your test set has an 'id' column, use it. Else, use a range.\n",
        "# if 'id' in test_data.columns:\n",
        "#     submission = pd.DataFrame({'id': test_data['id'], 'purchaseValue': test_predictions})\n",
        "# else:\n",
        "#     submission = pd.DataFrame({'id': range(len(test_predictions)), 'purchaseValue': test_predictions})\n",
        "\n",
        "# submission.to_csv('submission.csv', index=False)\n",
        "# print(\"Submission file 'submission.csv' created successfully.\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-30T14:36:03.184995Z",
          "iopub.execute_input": "2025-05-30T14:36:03.185227Z",
          "iopub.status.idle": "2025-05-30T14:49:04.873759Z",
          "shell.execute_reply.started": "2025-05-30T14:36:03.185208Z",
          "shell.execute_reply": "2025-05-30T14:49:04.872247Z"
        },
        "id": "f1_1Bi5crgEb"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# =======================\n",
        "# 1. Load Data\n",
        "# =======================\n",
        "train_data = pd.read_csv('/kaggle/input/engage-2-value-from-clicks-to-conversions/train_data.csv')\n",
        "test_data = pd.read_csv('/kaggle/input/engage-2-value-from-clicks-to-conversions/test_data.csv')\n",
        "\n",
        "# =======================\n",
        "# 2. Feature Selection & Engineering\n",
        "# =======================\n",
        "# Drop columns that are IDs, session identifiers, or not available at prediction time\n",
        "drop_cols = [\n",
        "    'purchaseValue', 'userId', 'sessionId', 'date', 'sessionStart', 'gclIdPresent'\n",
        "]\n",
        "drop_cols = [col for col in drop_cols if col in train_data.columns]\n",
        "\n",
        "# Convert device.isMobile to integer if present\n",
        "for df in [train_data, test_data]:\n",
        "    if 'device.isMobile' in df.columns:\n",
        "        df['isMobile'] = df['device.isMobile'].map({True:1, False:0, 'TRUE':1, 'FALSE':0, np.nan:0})\n",
        "\n",
        "# Prepare features and target\n",
        "y = np.log1p(train_data['purchaseValue'])\n",
        "X = train_data.drop(columns=drop_cols)\n",
        "X_test = test_data.drop(columns=[col for col in drop_cols if col != 'purchaseValue'])\n",
        "\n",
        "# Identify categorical and numeric columns\n",
        "categorical_features = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "numeric_features = X.select_dtypes(include=[np.number]).columns.tolist()\n",
        "\n",
        "# =======================\n",
        "# 3. Preprocessing Pipeline\n",
        "# =======================\n",
        "preprocessor = ColumnTransformer([\n",
        "    ('num', Pipeline([\n",
        "        ('imputer', SimpleImputer(strategy='median')),\n",
        "        ('scaler', StandardScaler())\n",
        "    ]), numeric_features),\n",
        "    ('cat', Pipeline([\n",
        "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "        ('encoder', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
        "    ]), categorical_features)\n",
        "])\n",
        "\n",
        "# =======================\n",
        "# 4. Model\n",
        "# =======================\n",
        "model = Pipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('regressor', XGBRegressor(\n",
        "        n_estimators=800,\n",
        "        max_depth=8,\n",
        "        learning_rate=0.03,\n",
        "        subsample=0.8,\n",
        "        colsample_bytree=0.8,\n",
        "        reg_alpha=0.1,\n",
        "        reg_lambda=0.1,\n",
        "        random_state=42,\n",
        "        n_jobs=1  # Set to 1 to avoid memory issues\n",
        "    ))\n",
        "])\n",
        "\n",
        "# =======================\n",
        "# 5. Cross-Validation (Optional, can skip for speed)\n",
        "# =======================\n",
        "kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
        "r2_scores = []\n",
        "for train_idx, val_idx in kf.split(X):\n",
        "    X_tr, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
        "    y_tr, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
        "    model.fit(X_tr, y_tr)\n",
        "    preds = model.predict(X_val)\n",
        "    score = r2_score(np.expm1(y_val), np.expm1(preds))\n",
        "    r2_scores.append(score)\n",
        "    print(f\"Fold R¬≤: {score:.4f}\")\n",
        "print(f\"Mean CV R¬≤: {np.mean(r2_scores):.4f}\")\n",
        "\n",
        "# =======================\n",
        "# 6. Train on Full Data & Predict\n",
        "# =======================\n",
        "model.fit(X, y)\n",
        "test_pred = np.expm1(model.predict(X_test))\n",
        "\n",
        "# =======================\n",
        "# 7. Submission\n",
        "# =======================\n",
        "id_col = 'userId' if 'userId' in test_data.columns else test_data.index\n",
        "submission = pd.DataFrame({\n",
        "    'id': range(0, test_data.shape[0]),\n",
        "    'purchaseValue': test_pred\n",
        "})\n",
        "submission.to_csv('submission.csv', index=False)\n",
        "print(\"Submission file 'submission.csv' created successfully.\")\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-06-01T20:54:12.214397Z",
          "iopub.execute_input": "2025-06-01T20:54:12.214815Z",
          "iopub.status.idle": "2025-06-01T21:14:44.876464Z",
          "shell.execute_reply.started": "2025-06-01T20:54:12.214789Z",
          "shell.execute_reply": "2025-06-01T21:14:44.875235Z"
        },
        "id": "aYLFqWVNrgEb"
      }
    }
  ]
}